"""LLM-backed regrouping of text chunks into coherent snippets."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, Iterable, Sequence


@dataclass(frozen=True)
class SegmentSuggestion:
    """Structured suggestion for a segment generated by an LLM."""

    text: str
    rationale: str | None = None


LLMClient = Callable[[str], str]


def regroup_chunks(
    *,
    chunk_texts: Sequence[str],
    llm_client: LLMClient,
    system_prompt: str,
    user_prompt_template: str,
) -> Iterable[SegmentSuggestion]:
    """Call an injected LLM client to regroup chunk texts into suggestions.

    ``llm_client`` is a callable that accepts a prompt and returns model output.
    The caller is responsible for parsing the model output into
    ``SegmentSuggestion`` objects.
    """

    if not chunk_texts:
        return []

    assembled_user_prompt = user_prompt_template.format(
        snippets="\n\n".join(chunk_texts)
    )
    response = llm_client(_build_prompt(system_prompt, assembled_user_prompt))
    return _parse_response(response)


def _build_prompt(system_prompt: str, user_prompt: str) -> str:
    return f"<system>\n{system_prompt}\n</system>\n<user>\n{user_prompt}\n</user>"


def _parse_response(response: str) -> Iterable[SegmentSuggestion]:
    # Placeholder: parse simple numbered list.
    suggestions: list[SegmentSuggestion] = []
    for line in response.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        if stripped[0].isdigit() and stripped[1:].startswith("."):
            suggestion_text = stripped.split(".", 1)[1].strip()
            suggestions.append(SegmentSuggestion(text=suggestion_text))
        else:
            suggestions.append(SegmentSuggestion(text=stripped))
    return suggestions
