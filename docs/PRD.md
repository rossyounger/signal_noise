# Signal/Noise PRD (Living Document)

**Objective:** Build a pipeline to Ingest → Segment → Save → Review → Label → Search audio and text content, culminating in structured Hypothesis testing and Analyst POVs.

## 1. Core Data Model

### Segments
- **Definition:** Atomic units of content (text/audio clips).
- **Key Fields:** `document_id`, `text`, `start_offset`, `end_offset`, `segment_status`.

### Hypotheses (Hypothesis-Centric Architecture)
- **`hypotheses`**: The primary entity - standalone, testable propositions.
    - `hypothesis_text`: The testable statement (e.g., "Google is falling behind in AI development").
    - `description`: Context or rationale for the hypothesis (can be multi-paragraph for complex papers).
    - `reference_url`: Optional link to external document (paper, article, book, website).
    - `reference_type`: Type of reference ('paper', 'article', 'book', 'website').
- **`hypothesis_evidence`**: Links segments to hypotheses as evidence.
    - `hypothesis_id`: Reference to the hypothesis.
    - `segment_id`: The segment providing evidence.
    - `verdict`: 'confirms', 'refutes', 'nuances', or 'irrelevant'.
    - `analysis_text`: AI/user explanation of the verdict.
    - `authored_by`: 'human' or 'ai'.
- **`hypothesis_reference_cache`**: Caches fetched external reference content for performance.
- **`questions`**: Navigation aids that link to relevant hypotheses.
    - `question_text`: A question users might ask.
- **`question_hypotheses`**: Many-to-many link between questions and hypotheses.
- **`persona_topic_povs`**: Synthesized high-level views generated by "Analyst" agents based on hypothesis evidence.

### Data Flow
```
Questions (optional navigation)
     ↓
  link to
     ↓
Hypotheses ←── hypothesis_evidence ←── Segments
  (core)            (verdicts)         (evidence)
```

## 2. Functional Requirements

### Phase 1: Ingestion & Segmentation (Completed)
- Ingest generic RSS feeds (Podcasts) and web pages (Stratechery).
- Transcribe audio to text (OpenAI/AssemblyAI).
- Store raw documents and enable manual segmentation (defining start/end points).
- **Segmentation Workbench:** Interactive UI for selecting text from HTML documents and creating segments with accurate HTML extraction using `find_html_fragment`.
- **Document Management:** Archive functionality to remove documents from active view.
- **Source Refresh:** Queue-based ingestion system for refreshing selected sources.

### Phase 2: Analysis Workflow (Completed - MVP)
- **User Interface:** Next.js application replacing Retool.
- **Workflow:**
    1. **Select Segment:** User picks a segment from the queue.
    2. **Hypothesis Association:**
        - System suggests existing relevant hypotheses.
        - System generates *new* potential hypotheses.
        - User can manually create hypotheses (with optional external references).
        - User can link existing hypotheses to the segment.
    3. **Evidence Analysis:**
        - AI analyzes segment against hypothesis to determine verdict (Confirms/Refutes/Nuances).
        - For hypotheses with external references, user can toggle "Use full reference document" for deep analysis.
        - Analysis modes: Summary-only (fast) or Full reference (comprehensive).
        - User can edit analysis text.
    4. **Save Evidence:**
        - User selects which hypotheses to save using checkboxes.
        - Saving writes a new evidence record to `hypothesis_evidence`.
- **Segmentation Workflow:**
    1. Navigate to document from Documents page.
    2. Click "Segment" button to open Segmentation Workbench.
    3. Select text in HTML viewer (selection persists visually).
    4. Click "Create Segment" to extract HTML and create segment entry.
    5. View existing segments in compact table above document content.
- **Questions (Navigation Feature):**
    - Create questions that serve as navigation aids.
    - Link multiple hypotheses to a question.
    - View all hypotheses linked to a question with their evidence counts.
    - Delete questions (preserves hypotheses, removes links only).
- **Hypothesis Management:**
    - Create hypotheses with optional external reference links (papers, articles, books).
    - View all evidence for a hypothesis.
    - Delete hypotheses (CASCADE deletes all evidence and links).
- **External Reference Support:**
    - Link hypotheses to external documents (PDF, web pages).
    - Automatic content fetching and caching for referenced documents.
    - User-controlled analysis depth (summary vs full document context).

### Phase 3: Synthesis (Pending/Next)
- **Analyst POV Agent:**
    - Input: A Hypothesis's evidence trail (all segments and verdicts).
    - Output: A cohesive "Point of View" summary.
    - Status: Endpoint exists (`POST /analysis:generate_pov`), logic is placeholder.

## 3. Technical Stack
- **Backend:** Python (FastAPI), Supabase (PostgreSQL + `pgvector` for future search).
- **Frontend:** Next.js (React), Tailwind CSS.
- **AI/ML:** OpenAI GPT-4o (via LangChain/LangGraph) for suggestions and analysis.

## 4. Design Guidelines
- **Navigation:** Simple top-bar navigation (Segments | Hypotheses | Questions | Documents | Sources).
- **Styling:** Clean, text-heavy interface. High contrast (black text on white).
- **Interactivity:**
    - Markdown editors for rich text.
    - "Run Evidence Analysis" provides immediate feedback loops.
    - Verdict badges (Confirms/Refutes/Nuances) for quick visual scanning.
    - Double confirmation prompts for destructive actions (delete).
